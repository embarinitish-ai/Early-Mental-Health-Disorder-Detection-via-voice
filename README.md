# üß† Early Mental Health Disorder Detection via Voice

**A deep learning pipeline for automated depression detection from speech audio using self-supervised speech representations.**

> **Binary classification** ‚Äî Healthy (0) vs. Depressed (1) ‚Äî from raw audio recordings, leveraging the acoustic biomarkers of Major Depressive Disorder (MDD): reduced pitch variability, monotone delivery, longer pause durations, and vocal tremors.

---

## Table of Contents

- [Abstract](#abstract)
- [Architecture Overview](#architecture-overview)
- [Datasets](#datasets)
- [Pipeline](#pipeline)
  - [Phase 1 ‚Äî Audio Preprocessing](#phase-1--audio-preprocessing)
  - [Phase 2 ‚Äî Metadata Generation & Data Splitting](#phase-2--metadata-generation--data-splitting)
  - [Phase 3 ‚Äî Model Training](#phase-3--model-training)
  - [Phase 4 ‚Äî Inference & Evaluation](#phase-4--inference--evaluation)
- [Model Architecture](#model-architecture)
- [Training Strategy](#training-strategy)
- [Environment Setup](#environment-setup)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Results](#results)
- [Key Design Decisions](#key-design-decisions)
- [References](#references)
- [License](#license)

---

## Abstract

Depression is one of the most prevalent mental health disorders worldwide, yet diagnosis remains largely subjective, relying on clinical interviews and self-report questionnaires. This project proposes an automated, non-invasive approach to depression detection by analyzing vocal biomarkers present in speech recordings. We fine-tune **Microsoft WavLM-base-plus**, a self-supervised speech representation model, using **Low-Rank Adaptation (LoRA)** combined with **selective layer unfreezing**, **multi-head attention temporal pooling**, and **Focal Loss** to achieve robust binary classification of depression from 10-second audio segments. The system aggregates chunk-level predictions into participant-level decisions via probability-weighted majority voting, making it suitable for real-world clinical screening scenarios.

---

## Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Raw Audio Recordings                        ‚îÇ
‚îÇ              (MODMA & DAIC-WOZ Datasets)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   preprocess.py     ‚îÇ  Phase 1: Audio Factory
              ‚îÇ  ‚Ä¢ Resample 16 kHz  ‚îÇ
              ‚îÇ  ‚Ä¢ Silence strip    ‚îÇ
              ‚îÇ  ‚Ä¢ Normalize [-1,1] ‚îÇ
              ‚îÇ  ‚Ä¢ Chunk 10s        ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ build_metadata.py   ‚îÇ  Phase 2: Metadata & Splitting
              ‚îÇ  ‚Ä¢ Assign labels    ‚îÇ
              ‚îÇ  ‚Ä¢ Balance classes  ‚îÇ
              ‚îÇ  ‚Ä¢ 80/20 split      ‚îÇ
              ‚îÇ  (participant-level)‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ     train.py        ‚îÇ  Phase 3: Fine-Tuning
              ‚îÇ  WavLM + LoRA       ‚îÇ
              ‚îÇ  + Attention Pool   ‚îÇ
              ‚îÇ  + Focal Loss       ‚îÇ
              ‚îÇ  + Mixup            ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ    predict.py       ‚îÇ  Phase 4: Inference
              ‚îÇ  Per-participant    ‚îÇ
              ‚îÇ  majority voting    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Datasets

This project utilizes two publicly available clinical depression speech corpora:

### 1. MODMA (Multi-modal Open Dataset for Mental-disorder Analysis)

| Property | Detail |
|---|---|
| **Source** | Lanzhou University, 2015 |
| **Language** | Mandarin Chinese |
| **Content** | Interview recordings |
| **Labeling** | Clinical diagnosis (MDD vs. Control) |
| **ID Convention** | `0201xxxx` = Healthy, `0202xxxx`/`0203xxxx` = Depressed |
| **Format** | WAV files organized by participant directory |

### 2. DAIC-WOZ (Distress Analysis Interview Corpus ‚Äî Wizard of Oz)

| Property | Detail |
|---|---|
| **Source** | University of Southern California |
| **Language** | English |
| **Content** | Semi-structured clinical interviews with virtual agent "Ellie" |
| **Labeling** | PHQ-8 binary score (PHQ-8 ‚â• 10 = Depressed) |
| **Reference** | AVEC 2017 Depression Sub-challenge |
| **Format** | Pre-segmented participant audio chunks |

> **Cross-lingual design**: By training on both Mandarin and English speech, the model learns language-agnostic acoustic depression markers rather than lexical content, improving generalizability.

---

## Pipeline

### Phase 1 ‚Äî Audio Preprocessing

**Script**: [`preprocess.py`](preprocess.py)

Transforms raw clinical audio into standardized, clean 10-second WAV chunks.

| Step | Description | Parameters |
|---|---|---|
| **Resampling** | Convert to 16 kHz mono | Target SR = 16,000 Hz |
| **Silence Stripping** | Remove silent intervals using energy-based VAD | `top_db=25`, gap threshold = 500 ms |
| **Peak Normalization** | Scale signal to [-1, 1] | Amplitude normalization |
| **Chunking** | Slice into fixed-length segments | 10 seconds (160,000 samples) |
| **Tail Discard** | Drop final chunk if too short | Minimum 1 second |

**Output**: `refined_data/[Dataset]_[ParticipantID]_[ChunkIndex].wav`

**Design choice**: Silence stripping preserves natural speech pauses ‚â§ 500 ms (which are themselves depression biomarkers) while removing long dead-air intervals that add no diagnostic value.

---

### Phase 2 ‚Äî Metadata Generation & Data Splitting

**Script**: [`build_metadata.py`](build_metadata.py)

Creates a master metadata CSV with ground-truth labels and participant-level train/validation splits.

**Key operations**:

1. **Label Assignment** ‚Äî Loads clinical labels from MODMA `.xlsx` and DAIC-WOZ AVEC2017 CSVs (`PHQ8_Binary`).
2. **Balanced Sampling** ‚Äî Downsamples the majority class at the participant level to ensure equal class representation.
3. **Participant-Level Splitting** ‚Äî 80/20 train/validation split using `GroupShuffleSplit` (seed = 42).
4. **Leakage Prevention** ‚Äî Strict assertion that no participant appears in both train and validation sets.

**Output**: `master_metadata.csv` with columns: `file_path`, `participant_id`, `dataset`, `label`, `split`

> **Critical**: All splits operate at the participant level, not the chunk level. This prevents data leakage where different chunks from the same speaker's recording could appear in both training and validation sets, which would artificially inflate performance.

---

### Phase 3 ‚Äî Model Training

**Script**: [`train.py`](train.py)

Fine-tunes WavLM-base-plus with a custom training pipeline optimized for depression detection.

See [Model Architecture](#model-architecture) and [Training Strategy](#training-strategy) for full details.

---

### Phase 4 ‚Äî Inference & Evaluation

**Script**: [`predict.py`](predict.py)

Performs participant-level depression prediction by aggregating chunk-level softmax probabilities.

**Inference pipeline**:

1. Load the saved WavLM + AttentionPool model
2. For each participant, run all their audio chunks through the model
3. Compute per-chunk softmax probabilities P(healthy) and P(depressed)
4. Average probabilities across all chunks for each participant
5. Apply decision threshold (default: 0.4) ‚Äî P(depressed) > threshold ‚Üí depressed

**Evaluation metrics**:
- Balanced Accuracy (primary metric)
- F1 Score, Accuracy
- Per-class Precision, Recall
- Confusion Matrix
- Threshold sensitivity analysis (0.30‚Äì0.50)

> **Lower threshold (0.4 vs. 0.5)**: In clinical screening, false negatives (missing a depressed patient) are more costly than false positives. A threshold of 0.4 biases toward higher recall for the depressed class.

---

## Model Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Raw Audio (10s, 16 kHz)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Wav2Vec2 Feature Extractor                  ‚îÇ
‚îÇ           (CNN encoder ‚Üí 499 √ó 768 frames)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              WavLM-base-plus Encoder                  ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ   Layers 0-7:  Frozen + LoRA adapters (rank=16)      ‚îÇ
‚îÇ                (preserve acoustic knowledge)          ‚îÇ
‚îÇ                                                       ‚îÇ
‚îÇ   Layers 8-11: Fully unfrozen                         ‚îÇ
‚îÇ                (learn task-specific representations)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Multi-Head Attention Pooling (2 heads)        ‚îÇ
‚îÇ   Learns which temporal frames carry diagnostic info  ‚îÇ
‚îÇ   (pauses, monotone, tremors) ‚Üí weighted summary     ‚îÇ
‚îÇ   Output: [B, 768]                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Classification Head                    ‚îÇ
‚îÇ   Dropout(0.15) ‚Üí Linear(768‚Üí256) ‚Üí ReLU             ‚îÇ
‚îÇ   ‚Üí LayerNorm ‚Üí Dropout(0.15) ‚Üí Linear(256‚Üí2)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components

| Component | Details |
|---|---|
| **Backbone** | `microsoft/wavlm-base-plus` ‚Äî 12 transformer layers, 768-dim hidden size, ~94.7M params |
| **LoRA** | Rank 16, Œ±=32, dropout 0.1 ‚Äî applied to Q and V projections in layers 0‚Äì7 |
| **Attention Pooling** | 2-head learnable attention with query/key/value projections + LayerNorm |
| **Classifier** | 2-layer MLP: 768 ‚Üí 256 ‚Üí 2 with ReLU, LayerNorm, and Dropout |

---

## Training Strategy

### Loss Function: Focal Loss

Standard cross-entropy treats all samples equally, but depression datasets often have ambiguous borderline cases. **Focal Loss** (Lin et al., 2017) down-weights well-classified examples and focuses on hard/misclassified samples:

$$\mathcal{L}_{FL} = -\alpha_t (1 - p_t)^\gamma \log(p_t)$$

- **Œ≥ = 2.0** ‚Äî Modulates focus on hard examples
- **Œ±** ‚Äî Set from inverse class frequencies for balance
- **Label smoothing = 0.05** ‚Äî Mild regularization

### Mixup Regularization

During training, 50% of batches apply Mixup (Zhang et al., 2018):

$$\tilde{x} = \lambda x_i + (1-\lambda) x_j, \quad \tilde{y} = \lambda y_i + (1-\lambda) y_j$$

- **Œ± = 0.2** ‚Äî Beta distribution parameter
- Œª ‚â• 0.5 enforced so the original sample dominates
- Smooths the decision boundary and prevents class flipping

### Layer-Wise Learning Rate Decay

Different layers capture different levels of abstraction and require different update magnitudes:

| Layer Group | Strategy | Learning Rate |
|---|---|---|
| Layers 0-7 | Frozen + LoRA | 5e-5 √ó 0.75¬≥ = 2.11e-5 |
| Layers 8-11 | Fully Unfrozen | 5e-5 √ó 0.75¬π = 3.75e-5 |
| Classifier Head | Fully Trainable | 5e-5 (full) |

### Data Augmentation (Gentle)

Augmentation is intentionally light to avoid destroying subtle depression markers:

| Technique | Parameters |
|---|---|
| Gaussian Noise | œÉ ‚àà [0.001, 0.005] |
| Random Gain | ¬±2 dB |
| Time Masking | 1‚Äì2 masks, 0.1‚Äì0.25s each |
| Circular Time Shift | ¬±0.2s (50% probability) |
| Frequency Masking (SpecAugment) | 100‚Äì4000 Hz band, 30% probability |

### Hyperparameters

| Parameter | Value | Rationale |
|---|---|---|
| Optimizer | AdamW | Decoupled weight decay |
| Base Learning Rate | 5e-5 | Conservative for pretrained model |
| LR Scheduler | Cosine with Restarts (2 cycles) | Avoids local minima |
| Warmup | 10% of total steps | Stable initialization |
| Weight Decay | 0.05 | Regularization for more trainable params |
| Max Gradient Norm | 1.0 | Prevents exploding gradients |
| Epochs | 40 | Extended training with restarts |
| Early Stopping Patience | 10 epochs | Based on balanced accuracy |
| Effective Batch Size | 32 (8 √ó 4 accumulation) | Smoother gradient estimates |
| FP16 | Enabled (if CUDA available) | Memory efficiency |
| Gradient Checkpointing | Enabled | Reduces VRAM usage |

---

## Environment Setup

### Prerequisites

- **OS**: Windows 10/11
- **Python**: 3.11
- **GPU**: NVIDIA GPU with CUDA 11.8 support (recommended: ‚â• 6 GB VRAM)
- **Conda**: Anaconda or Miniconda

### Automated Setup

```bash
# Run the setup script (creates "Nirvana" conda environment)
setup_env.bat
```

This installs:

```
conda create -n Nirvana python=3.11
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets accelerate librosa soundfile pandas openpyxl scikit-learn numpy peft
```

### Manual Setup

```bash
conda create -n Nirvana python=3.11 -y
conda activate Nirvana
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets accelerate librosa soundfile pandas openpyxl scikit-learn numpy peft
```

---

## Usage

### Step 1: Preprocess Audio

```bash
conda activate Nirvana
python preprocess.py
```

Scans `audio_lanzhou_2015/` (MODMA) and `DiacWoz/` (DAIC-WOZ), outputs clean chunks to `refined_data/`.

### Step 2: Build Metadata

```bash
python build_metadata.py
```

Generates `master_metadata.csv` with labels and participant-level train/val splits.

### Step 3: Train Model

```bash
python train.py
```

Fine-tunes WavLM with all training enhancements. Saves checkpoints and best model to `wavlm_lora_v10/`.

### Step 4: Run Inference

```bash
# Default threshold (0.4)
python predict.py

# Custom threshold
python predict.py --threshold 0.35

# Evaluate on training set
python predict.py --split train
```

---

## Project Structure

```
nir/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ setup_env.bat                # Environment setup script
‚îú‚îÄ‚îÄ preprocess.py                # Phase 1: Audio preprocessing
‚îú‚îÄ‚îÄ build_metadata.py            # Phase 2: Metadata & splitting
‚îú‚îÄ‚îÄ train.py                     # Phase 3: Model training (v10)
‚îú‚îÄ‚îÄ predict.py                   # Phase 4: Inference & evaluation
‚îú‚îÄ‚îÄ master_metadata.csv          # Generated metadata with splits
‚îÇ
‚îú‚îÄ‚îÄ audio_lanzhou_2015/          # Raw MODMA dataset
‚îÇ   ‚îú‚îÄ‚îÄ subjects_information_audio_lanzhou_2015.xlsx
‚îÇ   ‚îî‚îÄ‚îÄ [participant_dirs]/      # Raw WAV files per participant
‚îÇ
‚îú‚îÄ‚îÄ DiacWoz/                     # Raw DAIC-WOZ dataset
‚îÇ   ‚îú‚îÄ‚îÄ LAbles/                  # AVEC2017 label CSVs
‚îÇ   ‚îî‚îÄ‚îÄ Processed_Chunks/       # Pre-segmented audio
‚îÇ
‚îú‚îÄ‚îÄ refined_data/                # Preprocessed 10s audio chunks
‚îÇ   ‚îî‚îÄ‚îÄ [Dataset]_[PID]_[Idx].wav
‚îÇ
‚îî‚îÄ‚îÄ wavlm_lora_v10/             # Training outputs
    ‚îú‚îÄ‚îÄ best_model/              # Saved model weights
    ‚îú‚îÄ‚îÄ checkpoint-*/            # Training checkpoints
    ‚îú‚îÄ‚îÄ training_log.csv         # Per-epoch metrics log
    ‚îî‚îÄ‚îÄ participant_predictions.csv  # Inference results
```

---

## Results

### Evaluation Metrics

| Metric | Description |
|---|---|
| **Balanced Accuracy** | Average of per-class recall ‚Äî primary metric to handle class imbalance |
| **F1 Score** | Harmonic mean of precision and recall for the depressed class |
| **Per-Class Accuracy** | Independent accuracy for healthy and depressed classes |

### Monitoring During Training

Key indicators of healthy training:
- Balanced accuracy ‚â• 0.75 by epoch 10
- Both `acc_healthy` and `acc_depressed` > 0.65 (no class collapse)
- Stable progression without wild oscillation between classes

---

## Key Design Decisions

### 1. Why WavLM over Wav2Vec2?

WavLM is pre-trained with a denoising objective in addition to the masked speech prediction used by Wav2Vec2. This makes it more robust to recording quality variations present across the two datasets (different microphones, recording environments, and languages).

### 2. Why LoRA + Selective Unfreezing?

- **LoRA alone** (v8‚Äìv9): Only 1.1% of parameters trainable ‚Äî insufficient model capacity, accuracy plateaued at ~70%.
- **Full fine-tuning**: Risk of catastrophic forgetting of pre-trained knowledge and overfitting on small clinical datasets.
- **Hybrid approach** (v10): LoRA on lower layers (preserves acoustic features) + full unfreezing of top 4 layers (adapts semantic/emotional representations). ~4M additional trainable parameters.

### 3. Why Attention Pooling?

Statistical pooling (mean + std + max) treats all temporal frames equally. Depression biomarkers are localized to specific moments ‚Äî pauses, monotone segments, vocal tremors. Multi-head attention pooling learns to weight these informative frames more heavily during classification.

### 4. Why Focal Loss?

Weighted cross-entropy caused class oscillation: the model would alternate between overfitting to one class per epoch. Focal Loss provides a smoother training signal by automatically down-weighting easy examples, regardless of class.

### 5. Why Participant-Level Splitting?

A single speaker may have 15+ audio chunks. If chunks from the same speaker appear in both train and validation sets, the model can learn speaker identity rather than depression markers, leading to inflated metrics that do not generalize.

### 6. Why a Lower Prediction Threshold (0.4)?

In a clinical screening context, the cost of missing a depressed individual (false negative) outweighs the cost of a false alarm (false positive). A lower threshold increases recall for the depressed class at a small cost to precision.

---

## References

1. **WavLM**: Chen, S., et al. "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing." *IEEE JSTSP*, 2022.
2. **LoRA**: Hu, E. J., et al. "LoRA: Low-Rank Adaptation of Large Language Models." *ICLR*, 2022.
3. **Focal Loss**: Lin, T.-Y., et al. "Focal Loss for Dense Object Detection." *ICCV*, 2017.
4. **Mixup**: Zhang, H., et al. "mixup: Beyond Empirical Risk Minimization." *ICLR*, 2018.
5. **DAIC-WOZ**: Gratch, J., et al. "The Distress Analysis Interview Corpus of Human and Computer Interviews." *LREC*, 2014.
6. **MODMA**: Cai, H., et al. "A Multi-modal Open Dataset for Mental-disorder Analysis." *Scientific Data*, 2022.
7. **AVEC 2017**: Ringeval, F., et al. "AVEC 2017 ‚Äì Real-life Depression and Affect Recognition Workshop and Challenge." *ACM Multimedia*, 2017.

---

## License

This project is developed for academic and research purposes. The datasets used (MODMA and DAIC-WOZ) are subject to their own licensing agreements ‚Äî please ensure compliance with their respective data use agreements before use.

---

<p align="center">
  <i>Built with üéôÔ∏è PyTorch, ü§ó Transformers, and WavLM</i>
</p>
